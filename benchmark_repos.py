import os
import json
import time
from typing import List, Dict, Any
from config.config import settings as SETTINGS
from validator.validator import FastAPICodeValidator
from client.geminiclient import GeminiClient
from client.mistralclient import MistralClient
from client.openaiclient import OpenAIClient
from config.rules import RULES_STANDARD
from reports.charts_generator import generate_charts_report
from reports.statistic_report_generator import analyze, generate_report
from extraction.repo_collector import RepoCollector
from extraction.teste_colletor import TesteRepoCollector

def run_repo_benchmark():
    """
    Executa benchmark em reposit√≥rios reais do GitHub.
    """
    print("üöÄ Iniciando Benchmark de Reposit√≥rios Reais...")
    
    # 1. Configurar Reposit√≥rios Alvo
    repos = [
        "https://github.com/rednafi/fastapi-nano",
        "https://github.com/nsidnev/fastapi-realworld-example-app",
        #"https://github.com/tiangolo/full-stack-fastapi-template" # Muito grande/complexo para este teste r√°pido, descomentar se necess√°rio
    ]
    
    # 2. Configurar Clientes LLM
    clients = []
    
    if SETTINGS.GOOGLE_API_KEY:
        clients.append({
            "name": "Gemini 2.5-flash",
            "client": GeminiClient(api_key=SETTINGS.GOOGLE_API_KEY, model_name="gemini-2.5-flash"),
            "file_suffix": "gemini"
        })
        
    if SETTINGS.MISTRAL_API_KEY:
        clients.append({
            "name": "Mistral Small",
            "client": MistralClient(api_key=SETTINGS.MISTRAL_API_KEY, model_name="mistral-small-latest"),
            "file_suffix": "mistral"
        })
        
    # if SETTINGS.OPENAI_API_KEY:
    #     clients.append({
    #         "name": "GPT-4.1-nano",
    #         "client": OpenAIClient(api_key=SETTINGS.OPENAI_API_KEY, model_name="gpt-4.1-nano"),
    #         "file_suffix": "openai"
    #     })

    if not clients:
        print("‚ùå Nenhum cliente LLM configurado.")
        return

    results_summary = []

    # 3. Iterar sobre Reposit√≥rios
    for repo_url in repos:
        collector = TesteRepoCollector(repo_url)
        target_dir = collector.clone_repository()
        
        if not target_dir:
            continue
            
        print(f"\nüìÇ Analisando Reposit√≥rio: {collector.repo_name}")
        
        # Extrair endpoints usando a nova l√≥gica (parecido com RealWorldCollector)
        endpoints = collector.extract_endpoints()
        if not endpoints:
            print(f"‚ö†Ô∏è Nenhum endpoint encontrado em {collector.repo_name}.")
            collector.cleanup()
            continue
            
        # Prepara dicion√°rio para valida√ß√£o em batch
        # Mapeia ID -> C√≥digo
        endpoint_codes = { ep['id']: ep['code'] for ep in endpoints }
        
        # Iterar sobre Clientes para este Reposit√≥rio
        for item in clients:
            llm_name = item["name"]
            client_instance = item["client"]
            suffix = item["file_suffix"]
            
            # Check if result already exists
            filename = f"repo_results_{suffix}_{collector.repo_name}.json"
            if os.path.exists(filename):
                print(f"  ‚è≠Ô∏è  Skipping {llm_name} for {collector.repo_name} (Result exists: {filename})")
                results_summary.append({
                    "llm": f"{llm_name} ({collector.repo_name})",
                    "filename": filename,
                    "repo": collector.repo_name
                })
                continue
            
            print(f"  ü§ñ Validando {len(endpoints)} endpoints com {llm_name}...")
            start_time = time.time()
            
            # Validador
            validator = FastAPICodeValidator(llm_client=client_instance, rules=RULES_STANDARD)
            
            # Executa valida√ß√£o em batch nos endpoints extra√≠dos
            try:
                report = validator.validate_batch(endpoint_codes)
                
                # Adiciona metadados extras aos resultados 
                # (validate_batch usa o ID como file_path, vamos enriquecer se precisar)
                for res in report.get("results", []):
                    ep_id = res.get("file_path")
                    # Encontra o endpoint original para pegar mais metadados se quiser
                    orig_ep = next((e for e in endpoints if e['id'] == ep_id), None)
                    if orig_ep:
                        res["metadata"] = {**res.get("metadata", {}), **orig_ep["metadata"]}
                        res["source_file"] = orig_ep["source"]

                # Metadados do Benchmark
                report["benchmark_metadata"] = {
                    "llm_name": llm_name,
                    "repo_name": collector.repo_name,
                    "repo_url": repo_url,
                    "total_time": round(time.time() - start_time, 2),
                    "total_endpoints": len(endpoints)
                }
                
                # Salvar resultado
                filename = f"repo_results_{suffix}_{collector.repo_name}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                
                print(f"‚úÖ Conclu√≠do ({report['benchmark_metadata']['total_time']}s). Salvo: {filename}")
                
                results_summary.append({
                    "llm": f"{llm_name} ({collector.repo_name})",
                    "filename": filename,
                    "repo": collector.repo_name
                })
                
            except Exception as e:
                print(f"‚ùå Erro ao validar com {llm_name}: {e}")
        
        # Limpeza ap√≥s todos os LLMs rodarem neste repo
        collector.cleanup()

    print("\nüèÅ Benchmark de Reposit√≥rios Finalizado!")
    
    # Gera relat√≥rios finais (Reaproveitando geradores existentes)
    # Nota: generate_charts_report pode precisar de ajuste se esperar estrutura exata do benchmark sint√©tico,
    # mas vamos tentar usar. Para an√°lise estat√≠stica funciona.
    
    print("\nüìä Gerando Relat√≥rios Consolidados...")
    try:
        analyze(results_summary)
        generate_report(results_summary)
        generate_charts_report(file_pattern="repo_results_*.json", output_file="benchmark_repos_report.html")
    except Exception as e:
        print(f"‚ö†Ô∏è Erro na gera√ß√£o de relat√≥rio final: {e}")

if __name__ == "__main__":
    run_repo_benchmark()
